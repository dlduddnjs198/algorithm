**일리노이대학교 어바나-샴페인**과 **UMass 애머스트** 연구진이 ‘SEARCH-R1’이라는 새로운 LLM 추론 구조를 제안하였다.

이 모델은 기존의 **RAG나 프롬프트 방식 없이**, **LLM이 직접 검색 쿼리를 생성**하고 **검색 결과를 추론에 통합했다고 한다.**

### 기존 LLM의 한계점

- **RAG**: 유연성 부족, 대량의 지도 학습 필요.
- **프롬프트 기반 검색 유도**: 검색 도구 사용법을 LLM이 스스로 학습하지 못함.
- **도구 기반 멀티턴 검색**: LLM이 능동적으로 활용하기 어려움.

### SEARCH-R1의 특징

- **단계적 추론 구조 (Chain-of-Thought 기반)**:  
  Think → Search → Information → Answer 구조로 동작.

- **자체적으로 검색 쿼리를 생성하고 실행**한 뒤, 그 결과를 다음 추론 단계에 반영.

- **순수 강화학습(RL)** 기반 학습:  
  인간 주석 없이도 LLM이 검색과 추론 방식을 스스로 학습.  
  **결과 기반 보상 모델** 사용: 최종 정답의 정확성만 평가하여 보상.

즉, 요약하자면, **기존에는 사람이 검색을 붙여주는 보조적 구조(RAG)였다면**, 이제는 **LLM 스스로 정보 탐색 능력을 가지게 된 것**이다. 이건 단순한 보조도구에서 **자율적 AI 에이전트로의 진화로의 가능성을 의미한다.**

**이는 RAG의 대안**으로, 기업에서 사용되는 검색 기반 LLM 활용 방식에 **중대한 변화로 보여질 여지가 있다.**

코드는 오픈소스로 GitHub에서 확인할 수 있다.

출처 : [https://www.aitimes.com/news/articleView.html?idxno=168937](https://www.aitimes.com/news/articleView.html?idxno=168937)

---

- **모를법한 내용**
<details> <summary>🧠 <strong>LLM (Large Language Model)</strong></summary>
수많은 텍스트 데이터를 학습해서 인간처럼 자연스럽게 언어를 생성하거나 이해할 수 있는 초거대 언어 모델이다. (예시 : ChatGPT, Claude, Gemini)

이러한 모델들을 통해서 질문에 답하고 글 요약, 코드 작성 등을 수행할 수 있다.

**📌 단점**<br>
LLM은 학습한 시점 이후의 최신 정보는 알 수 없다.
예: "2025년 4월 대통령이 누구야?" 같은 질문에는 답하지 못함.

**📌 이를 해결하기 위한 보완 방법**<br>

- RAG (Retrieval-Augmented Generation)
- Fine-tuning with Fresh Data (최신 정보로 재학습)
- Tool 사용 (API/검색 엔진 등 외부 도구 연동)

**📌 LLM 학습과정: 3단계**

LLM(대형 언어 모델)은 크게 보면 아래 **세 가지 훈련 단계**를 거쳐 만들어진다.

**🧠 ① 사전 학습 (Pretraining)**

- **대량의 인터넷 텍스트**를 이용해서 "다음 단어 맞추기" 훈련을 함.
- 예: “고양이는 귀엽고 ___를 좋아한다” → 물고기

→ 이건 **자기지도학습(Self-supervised Learning)**이라고 해서, 정답(label)을 사람이 직접 붙이지 않아도 텍스트 자체로 학습함.

---

**🗣️ ② 지도 미세조정 (Supervised Fine-tuning, SFT)**

- 인간이 만든 **질문-답변 세트, 요약 결과 등** 정답이 포함된 데이터를 이용해 훈련.
- 예:
    - 입력: “한국의 수도는?”
    - 정답: “서울입니다.”

→ 이 단계가 있으면 **정확한 응답**을 잘 하게 된다.

하지만 **제작비가 많이 듦.**

---

**🙋 ③ RLHF (Reinforcement Learning from Human Feedback)**

- 모델이 생성한 여러 응답 중 인간이 “이게 제일 좋아”라고 선택 → 그걸 기준으로 **보상함수**를 만들어서 강화학습 적용.

→ ChatGPT의 “말투가 부드럽고 자연스럽다”는 이유도 여기에 있음.

→ 단점: **사람이 계속 피드백을 줘야 하고, 주관적임.**

</details>
<details> <summary>🔎 <strong>RAG (Retrieval-Augmented Generation)</strong></summary>
LLM이 답을 생성할 때, **외부 지식을 실시간으로 검색해서 활용하는 방식**이다.

여기서 검색은 추론 “전에” 일어나는 절차이다. LLM은 받은 내용을 활용만 한다.(검색 자체를 스스로 하지는 않는다는 뜻)

**구조는 다음과 같다.**

```
1. 사용자의 질문 →
2. 검색 시스템이 관련 문서를 찾아옴 (Retrieval) →
3. LLM이 그 문서를 읽고 답변을 생성함 (Generation)
```

예:

> ❓ "이순신 장군의 출생지는 어디야?"
>
>
> 🔍 검색: “서울시 종로구 건천동”이라는 문서 찾음
>
> 🤖 GPT가 이 문서를 읽고 → “이순신 장군은 서울 건천동에서 태어났습니다.” 생성
>

**이러한 RAG 기법은 대부분의 기업용 AI 챗봇 등에서 도입되고 있다. (e.g. 사내 지식검색, 뉴스 요약, 고객 FAQ 등)**

RAG의 장단점은 다음과 같다.

| 구분 | 내용                                                                                            |
| --- |-----------------------------------------------------------------------------------------------|
| ✅ 장점 | - **최신 정보 활용 가능** (인터넷/DB에서 실시간 검색)<br>- 학습 없이도 빠르게 구축 가능<br> - 도메인별 문서를 넣어서 **특화된 응답 생성 가능** |
| ❌ 단점 | - 검색 품질에 따라 LLM 응답 품질도 영향 받음                                                                  
- **검색 결과를 LLM에게 “어떻게 넣을지” 구조 설계가 필요함** (context window 문제)
- LLM이 **검색을 능동적으로 결정하지 못함** (프롬프트에 의존) |
- 핵심은 "**LLM이 생성 전에, 외부 지식 기반 문서를 검색해서 참고하도록 하는 구조**"이다.
- 이 외부 지식은 보통 사전에 **수집해둔 문서나 벡터DB**에 저장되어 있다.
- 즉, "내가 갖고 있는 자료 중에서 찾아봐" → 찾아서 LLM에게 넣어주고 → 그걸 읽고 답변을 생성한다.

**예시**

```
- 사내 지식베이스(R&D 문서, 기술 매뉴얼 등)를 벡터로 인코딩해두고,
- 질문이 들어오면 유사 문서를 찾아 → LLM이 답변 생성에 활용함.
```

> 예를 들어, 회사에서 100만 개의 고객 FAQ 문서를 가지고 있다고 하자.
>
>
> 이걸 벡터화해서 **벡터DB에 저장**해두고,
>
> 사용자가 질문하면 **유사 문서 3~5개를 검색해서 LLM에게 넣어줌**
>
> → LLM이 그걸 기반으로 답을 생성.
>

이때 검색된 문서는 OpenAI나 Bing이 검색한 게 아니라,

내가 미리 준비해둔 "신뢰할 수 있는 문서 모음"이다.
</details>
<details> <summary>🧩 <strong>추론 (Inference)</strong></summary>
📌 AI 전반에서의 의미
학습이 끝난 모델이 새로운 입력에 대해 예측이나 판단을 내리는 실행 단계
즉, 훈련 이후 실제로 사용하는 단계.

📌 예시

이미지 분류: 고양이 사진 → “고양이” 예측

음성 인식: "안녕" → “안녕” 텍스트로 변환

자율주행: 카메라 입력 → 차선 감지 → 조향 판단

📌 LLM에서의 의미
LLM에서는 "단어를 하나씩 생각하며 생성해나가는 과정"이 바로 추론이다.

예:
질문: “마라톤을 완주하려면 어떻게 준비해야 해?”

마라톤 배경 지식 활성화

훈련 계획, 식단 recall

토큰을 하나씩 생성해 응답 구성

📌 특히 CoT (Chain-of-Thought)에서는 다음과 같이 작동:

“이건 모르겠는데 → 검색이 필요해 → 정보를 얻었어 → 다시 생각”

→ 이런 일련의 사고 흐름을 추론 과정이라고 부른다.

</details>
<details> <summary>🎯 <strong>RLHF (Reinforcement Learning from Human Feedback)</strong></summary>
인간의 피드백을 이용하여 강화 학습 에이전트를 훈련시키는 방법이다.
AI 시스템이 인간의 선호도에 맞춰 더 자연스럽고 적절하게 동작하도록 훈련하는 데 사용된다.

대표적으로 ChatGPT 훈련의 마지막 단계로 사용된다.

📌 어떻게 작동하나?

LLM이 하나의 질문에 대해 여러 개의 응답을 생성

인간 평가자가 가장 좋은 응답을 선택

그 선택을 기준으로 보상 모델(reward model)을 생성

보상 모델을 이용해 강화학습(RL) 적용

→ LLM이 보상을 많이 받는 방향으로 응답을 생성하도록 학습

📌 효과

말투가 더 자연스러워짐

유해한 표현이나 부적절한 응답을 줄일 수 있음

사용자 친화적 응답 품질 개선

📌 단점

인간 피드백 수집이 노동집약적이고 비용이 큼

평가 기준이 주관적일 수 있음

</details>
<details> <summary>🗣️ <strong>NLP (Natural Language Processing)</strong></summary>
**자연어처리(NLP)**는 컴퓨터가 인간의 언어(텍스트나 음성)를 이해하고 처리할 수 있도록 하는 인공지능 분야이다.

📌 활용 분야

챗봇, 가상 비서

기계 번역

검색 엔진

감정 분석

📌 어떻게 작동하나?
NLP 시스템은 대부분 기계 학습 / 딥러닝 기반의 모델을 사용하여
언어를 처리하고 분석한다.
문장 구조, 의미, 감정 등을 파악해 인간과 소통하는 시스템을 만든다.

📌 예시

가상 비서: “날씨 알려줘” → “오늘은 맑고 22도입니다.”

기계 번역: “Hello” → “안녕하세요”

검색 엔진: “강아지 사료 추천” → 관련 문서 탐색

감정 분석: “이 제품 별로였어요” → 부정적 감정 분류

📌 중요성
NLP는 인간-컴퓨터 간 자연스러운 소통을 가능하게 하며
다양한 산업에서 고객 응대, 데이터 분석, 문서 요약 등 핵심 기술로 활용된다.

</details>
<details> <summary>🧵 <strong>CoT (Chain of Thought, 사고 사슬)</strong></summary>
LLM이 정답을 바로 출력하지 않고, 생각 과정을 단계별로 풀며 추론하게 하는 방식이다.
이 구조를 적용하면 특히 논리적 문제나 수학 문제에서 정답률이 크게 올라간다.

📌 예시 문제

“철수는 사과 3개, 영희는 2개 가지고 있고, 둘이 합치면 몇 개?”

✖️ 일반 방식:
→ “정답은 5입니다.”

✔️ CoT 방식:

“철수는 사과 3개를 가지고 있다.”

“영희는 사과 2개를 가지고 있다.”

“둘이 합치면 3 + 2 = 5개다.”

“따라서 정답은 5입니다.”

📌 왜 쓰는가?

복잡한 문제에서 단계별 사고 과정이 정확도를 향상시킴

중간 사고 단계를 드러내면 오류를 추적하거나 수정하기도 쉬움

📌 적용 예시

수학 문제, 논리 퍼즐, 긴 문장 요약, 복잡한 질문 응답 등

GPT, Claude 등 최신 LLM들이 CoT 프롬프트를 통해 reasoning 능력을 강화함

</details>

<details> <summary>📚 <strong>세그먼트 (Segment)</strong></summary>
세그먼트는 "문장을 기능별로 나눈 조각"을 의미한다.
특히 SEARCH-R1이나 Chain-of-Thought(CoT) 구조에서는 추론 과정을 논리적 블록 단위로 나누기 위해 사용된다.

📌 예시:

txt
복사
편집
<think> 철수는 사과 3개를 가지고 있다. </think>
<search> "영희는 사과 몇 개?" </search>
<information> 영희는 2개를 가지고 있다. </information>
<answer> 따라서 총 5개입니다. </answer>
<think>: 내부적인 생각

<search>: 검색 필요성 판단 및 쿼리 생성

<information>: 검색 결과 삽입

<answer>: 최종 응답

이런 태그나 논리 단위를 각각 하나의 세그먼트라고 부른다.

</details>
<details> <summary>🎯 <strong>보상 모델 (Reward Model)</strong></summary>
강화학습(RL)에서 LLM의 출력이 좋은지 나쁜지를 평가하는 역할을 하는 모델이다.
LLM은 자체적으로 "응답이 좋은가?"를 판단하지 못하기 때문에, 보상 모델이 대신 **점수(보상)**를 부여해준다.

📌 예시:

질문: “서울 날씨 알려줘”

A 응답: “오늘 서울은 흐리고 15도입니다.” → 👍 좋은 응답 → 보상 0.9

B 응답: “서울은 대한민국의 수도입니다.” → 👎 엉뚱한 응답 → 보상 0.2

→ 이렇게 점수를 매기는 기준을 보상 모델이 제공한다.
이 점수를 바탕으로 LLM은 보상이 높은 응답을 더 잘 생성하도록 학습된다.

📌 어떻게 만드는가?

사람이 응답을 비교 평가한 데이터를 수집

어떤 응답이 더 나은지를 기준으로 ML 모델을 학습

이 모델이 LLM의 응답에 점수를 부여하는 역할을 수행

</details>
<details> <summary>🛠️ <strong>지도 미세조정 (SFT, Supervised Fine-Tuning)</strong></summary>
사전 학습된 LLM에 사람의 정답이 포함된 데이터를 추가로 학습시켜 성능을 향상시키는 단계이다.
정확도나 응답 품질을 높이기 위해 꼭 필요한 과정이다.

📌 왜 필요한가?

기본 LLM은 문법은 자연스러워도 사실이 틀릴 수 있음

사람이 직접 작성한 정답 예시를 통해 정확한 응답을 하도록 보정하는 목적

📌 예시 학습 데이터:

질문	정답
“파이썬에서 리스트 길이 구하는 법은?”	“len(리스트이름)을 사용하면 됩니다.”

→ 이런 데이터셋을 수십만~수백만 개 학습

📌 효과

“어떤 응답이 이상적인가?”에 대한 기준을 LLM이 학습

단순한 지식 전달을 넘어서 행동 방식이나 포맷까지 맞추게 됨

</details>

<details> <summary>🔡 <strong>토큰 (Token)</strong></summary>
"문자열(텍스트)을 의미 있는 단위로 쪼갠 것"을 의미한다.
용도에 따라 다양한 분야에서 사용된다.

📘 자연어처리(NLP) / LLM 기준
의미: 모델이 텍스트를 처리하기 위해 분해한 최소 단위

단어, 형태소, 음절, 글자 등 다양함

예:
"나는 바나나를 좋아해" → ["나는", " 바", "나", "나", "를", " 좋아", "해"]

GPT, BERT 등의 모델은 텍스트를 토큰 단위로 쪼개고 이를 숫자 벡터로 변환해서 처리함.
이 과정을 **토크나이징(tokenizing)**이라고 한다.

🧑‍💻 프로그래밍 언어 처리 (컴파일러 등)
의미: 소스코드를 의미 있는 단위로 나눈 결과물

예:
int x = 5 + 3; → ["int", "x", "=", "5", "+", "3", ";"]

🔐 보안/웹 인증에서의 토큰
의미: 인증 정보를 담은 디지털 문자열

예: JWT (JSON Web Token), OAuth Access Token

여기서의 토큰은 인증을 위한 임시 키, 사용자 신원 확인 등에 사용된다.

</details>
<details> <summary>🔍 <strong>검색엔진 (Search Engine)</strong></summary>
사용자의 질문이나 키워드를 입력하면 관련된 정보를 찾아주는 시스템이다.

📌 예시 (웹 검색 엔진)

구글(Google)

네이버(Naver)

빙(Bing)

DuckDuckGo 등

📌 내부 동작 구조

크롤링: 웹 페이지를 수집

인덱싱: 검색 속도를 위해 구조화

질의 처리: 사용자의 쿼리 해석

랭킹 정렬: 관련도 높은 순으로 결과 제공

검색엔진은 LLM이 정보를 얻기 위한 외부 지식 소스 또는 도구로도 활용된다.

</details>
<details> <summary>🔁 <strong>멀티턴 검색 (Multi-turn Search)</strong></summary>
여러 번 검색을 반복하면서 점점 정확한 정보를 찾는 방식을 의미한다.
단일 검색이 아닌, 질문-응답을 여러 차례 주고받으며 정보의 정밀도를 높이는 과정이다.

📌 예시 (대화형 검색 흐름)

사용자: “세계에서 가장 큰 동물은 뭐야?”

LLM: “대왕고래야.”

사용자: “그 동물의 평균 무게는?”

LLM: “평균 150톤 정도야.”

→ 이렇게 한 번에 답하지 않고 단계별로 질문을 이어가며 정보를 보완해나가는 방식이다.

RAG나 SEARCH-R1 같은 LLM 구조에서는 멀티턴 검색 기능이 있느냐가 중요한 성능 요소가 된다.

</details>

