**일리노이대학교 어바나-샴페인**과 **UMass 애머스트** 연구진이 ‘SEARCH-R1’이라는 새로운 LLM 추론 구조를 제안하였다.

이 모델은 기존의 **RAG나 프롬프트 방식 없이**, **LLM이 직접 검색 쿼리를 생성**하고 **검색 결과를 추론에 통합했다고 한다.**

### 기존 LLM의 한계점

- **RAG**: 유연성 부족, 대량의 지도 학습 필요.
- **프롬프트 기반 검색 유도**: 검색 도구 사용법을 LLM이 스스로 학습하지 못함.
- **도구 기반 멀티턴 검색**: LLM이 능동적으로 활용하기 어려움.

### SEARCH-R1의 특징

- **단계적 추론 구조 (Chain-of-Thought 기반)**:  
  Think → Search → Information → Answer 구조로 동작.

- **자체적으로 검색 쿼리를 생성하고 실행**한 뒤, 그 결과를 다음 추론 단계에 반영.

- **순수 강화학습(RL)** 기반 학습:  
  인간 주석 없이도 LLM이 검색과 추론 방식을 스스로 학습.  
  **결과 기반 보상 모델** 사용: 최종 정답의 정확성만 평가하여 보상.

즉, 요약하자면, **기존에는 사람이 검색을 붙여주는 보조적 구조(RAG)였다면**, 이제는 **LLM 스스로 정보 탐색 능력을 가지게 된 것**이다. 이건 단순한 보조도구에서 **자율적 AI 에이전트로의 진화로의 가능성을 의미한다.**

**이는 RAG의 대안**으로, 기업에서 사용되는 검색 기반 LLM 활용 방식에 **중대한 변화로 보여질 여지가 있다.**

코드는 오픈소스로 GitHub에서 확인할 수 있다.

출처 : [https://www.aitimes.com/news/articleView.html?idxno=168937](https://www.aitimes.com/news/articleView.html?idxno=168937)

---

### **모를법한 내용**
<details> <summary>🧠 <strong>LLM (Large Language Model)</strong></summary>
수많은 텍스트 데이터를 학습해서 인간처럼 자연스럽게 언어를 생성하거나 이해할 수 있는 초거대 언어 모델이다. (예시 : ChatGPT, Claude, Gemini)

이러한 모델들을 통해서 질문에 답하고 글 요약, 코드 작성 등을 수행할 수 있다.

**📌 단점**<br>
LLM은 학습한 시점 이후의 최신 정보는 알 수 없다.
예: "2025년 4월 대통령이 누구야?" 같은 질문에는 답하지 못함.

**📌 이를 해결하기 위한 보완 방법**<br>

- RAG (Retrieval-Augmented Generation)
- Fine-tuning with Fresh Data (최신 정보로 재학습)
- Tool 사용 (API/검색 엔진 등 외부 도구 연동)

**📌 LLM 학습과정: 3단계**

LLM(대형 언어 모델)은 크게 보면 아래 **세 가지 훈련 단계**를 거쳐 만들어진다.

**🧠 ① 사전 학습 (Pretraining)**

- **대량의 인터넷 텍스트**를 이용해서 "다음 단어 맞추기" 훈련을 함.
- 예: “고양이는 귀엽고 ___를 좋아한다” → 물고기

→ 이건 **자기지도학습(Self-supervised Learning)**이라고 해서, 정답(label)을 사람이 직접 붙이지 않아도 텍스트 자체로 학습함.

---

**🗣️ ② 지도 미세조정 (Supervised Fine-tuning, SFT)**

- 인간이 만든 **질문-답변 세트, 요약 결과 등** 정답이 포함된 데이터를 이용해 훈련.
- 예:
    - 입력: “한국의 수도는?”
    - 정답: “서울입니다.”

→ 이 단계가 있으면 **정확한 응답**을 잘 하게 된다.

하지만 **제작비가 많이 듦.**

---

**🙋 ③ RLHF (Reinforcement Learning from Human Feedback)**

- 모델이 생성한 여러 응답 중 인간이 “이게 제일 좋아”라고 선택 → 그걸 기준으로 **보상함수**를 만들어서 강화학습 적용.

→ ChatGPT의 “말투가 부드럽고 자연스럽다”는 이유도 여기에 있음.

→ 단점: **사람이 계속 피드백을 줘야 하고, 주관적임.**

</details>
<details> <summary>🔎 <strong>RAG (Retrieval-Augmented Generation)</strong></summary>
LLM이 답을 생성할 때, **외부 지식을 실시간으로 검색해서 활용하는 방식**이다.

여기서 검색은 추론 “전에” 일어나는 절차이다. LLM은 받은 내용을 활용만 한다.(검색 자체를 스스로 하지는 않는다는 뜻)

**구조는 다음과 같다.**

```
1. 사용자의 질문 →
2. 검색 시스템이 관련 문서를 찾아옴 (Retrieval) →
3. LLM이 그 문서를 읽고 답변을 생성함 (Generation)
```

예:

> ❓ "이순신 장군의 출생지는 어디야?"
>
>
> 🔍 검색: “서울시 종로구 건천동”이라는 문서 찾음
>
> 🤖 GPT가 이 문서를 읽고 → “이순신 장군은 서울 건천동에서 태어났습니다.” 생성
>

**이러한 RAG 기법은 대부분의 기업용 AI 챗봇 등에서 도입되고 있다. (e.g. 사내 지식검색, 뉴스 요약, 고객 FAQ 등)**

RAG의 장단점은 다음과 같다.

| 구분 | 내용                                                                                            |
| --- |-----------------------------------------------------------------------------------------------|
| ✅ 장점 | - **최신 정보 활용 가능** (인터넷/DB에서 실시간 검색)<br>- 학습 없이도 빠르게 구축 가능<br> - 도메인별 문서를 넣어서 **특화된 응답 생성 가능** |
| ❌ 단점 | - 검색 품질에 따라 LLM 응답 품질도 영향 받음                                                                  
- **검색 결과를 LLM에게 “어떻게 넣을지” 구조 설계가 필요함** (context window 문제)
- LLM이 **검색을 능동적으로 결정하지 못함** (프롬프트에 의존) |
- 핵심은 "**LLM이 생성 전에, 외부 지식 기반 문서를 검색해서 참고하도록 하는 구조**"이다.
- 이 외부 지식은 보통 사전에 **수집해둔 문서나 벡터DB**에 저장되어 있다.
- 즉, "내가 갖고 있는 자료 중에서 찾아봐" → 찾아서 LLM에게 넣어주고 → 그걸 읽고 답변을 생성한다.

**예시**

```
- 사내 지식베이스(R&D 문서, 기술 매뉴얼 등)를 벡터로 인코딩해두고,
- 질문이 들어오면 유사 문서를 찾아 → LLM이 답변 생성에 활용함.
```

> 예를 들어, 회사에서 100만 개의 고객 FAQ 문서를 가지고 있다고 하자.
>
>
> 이걸 벡터화해서 **벡터DB에 저장**해두고,
>
> 사용자가 질문하면 **유사 문서 3~5개를 검색해서 LLM에게 넣어줌**
>
> → LLM이 그걸 기반으로 답을 생성.
>

이때 검색된 문서는 OpenAI나 Bing이 검색한 게 아니라,

내가 미리 준비해둔 "신뢰할 수 있는 문서 모음"이다.
</details>
<details> <summary>🧩 <strong>추론 (Inference)</strong></summary>
LLM 기준으로는 좀더 구체적인 개념이다. LLM 기준으로는 **LLM이 텍스트를 한 토큰씩 "생각하며 생성해나가는 과정”을 의미한다.**

**예시:**

질문: “마라톤을 완주하려면 어떻게 준비해야 해?”

LLM 내부에서는:

- 마라톤 관련 배경 지식 활성화
- 체력 훈련, 식단, 러닝 계획 등 내용 recall
- 단어 하나하나 확률적으로 선택하며 텍스트 생성

→ 이 일련의 과정이 LLM 관점에서의 "**추론**"이야.

📌 특히 CoT(Chain-of-Thought) 문맥에서는:

- LLM이 **내부적으로 단계적 reasoning**을 수행하며
- **“이건 모르겠는데 → 검색 필요해 → 정보를 얻었어 → 다음 단계를 생각해보자”**
- → 이런 **사고 흐름 자체**를 **"추론"이라고 부른다.**
</details>
<details> <summary>🎯 <strong>RLHF (Reinforcement Learning from Human Feedback)</strong></summary>
**인간의 피드백을 이용하여 강화 학습 에이전트를 훈련시키는 방법이다.**

**이 방법은 AI 시스템을 인간의 선호도에 맞춰 더 잘 동작하도록 훈련하는 데 사용된다. 대표적으로 ChatGPT 훈련의 마지막 단계이다.**
</details>
<details> <summary>🗣️ <strong>NLP (Natural Language Processing)</strong></summary>
**컴퓨터가 인간의 언어를 이해하고 처리할 수 있도록 하는 인공지능의 한 분야이다**. 이는 컴퓨터가 자연어 텍스트 또는 음성으로 데이터를 상호 연결하는 기술을 의미합니다.

이는 챗봇, 가상 비서, 번역 서비스, 검색 엔진 등 다양한 분야에서 활용된다.

NLP는 **주로 기계 학습과 딥 러닝을 이용하여 학습하고 예측하는 모델을 구축**한다.

NLP는 **텍스트를 분석하고 이해하여 문맥, 의미, 감정 등을 파악하는 데 사용**된다.

**NLP의 예시:**

- **가상 비서(챗봇) :** 사용자의 질문을 이해하고 자연어로 답변하는 챗봇.
- **기계 번역 :** 언어 장벽을 허물고 다른 언어로 번역하는 서비스.
- **검색 엔진 :** 검색어와 관련한 정보를 찾고 제공하는 검색 엔진.
- **정서 분석 :** 고객의 리뷰나 소셜 미디어 게시글에서 감정이나 의견을 파악하는 분석.

**NLP의 중요성:**

NLP는 인간과 컴퓨터 간의 소통을 더욱 편리하고 효율적으로 만들어주는 기술이다. 다양한 분야에서 NLP를 활용하여 새로운 서비스와 기능을 개발할 수 있다.
</details>
<details> <summary>🧵 <strong>CoT (Chain of Thought, 사고 사슬)</strong></summary>
**AI가 문제를 바로 푸는 게 아니라, 생각 과정을 단계별로 풀어서** 추론하게 만드는 방식이다.

**📌 예시:**

**문제:** "철수는 사과 3개, 영희는 2개 가지고 있고, 둘이 합치면 몇 개?"

- ✖️ 일반 방식: “정답은 5입니다.”
- ✔️ CoT 방식:
  1. “철수는 사과 3개를 가지고 있다.”
  2. “영희는 사과 2개를 가지고 있다.”
  3. “둘이 합치면 3 + 2 = 5개다.”
  4. “따라서 정답은 5입니다.”

---

**📌 왜 쓰는가?**

- **복잡한 문제일수록 단계를 나눠서 사고하면 정확도가 올라가기 때문**이다.
- 실제로 CoT 프롬프트를 쓰면 LLM의 수학, 논리 문제 정답률이 **확 뛰어오르는 효과**가 있다.
</details>
<details> <summary>📚 <strong>세그먼트 (Segment)</strong></summary>
**세그먼트는 말 그대로 “문장을 기능별로 나눈 조각”을 말한다.**

SEARCH-R1이나 CoT 방식에서는 추론 과정을 **논리적인 블록**으로 나눠서 관리한다. 예를 들면:

**예시:**

```
<think> 철수는 사과 3개를 가지고 있다. </think>
<search> "영희는 사과 몇 개?" </search>
<information> 영희는 2개를 가지고 있다. </information>
<answer> 따라서 총 5개입니다. </answer>
```

→ **이런 태그나 구간 하나하나가 세그먼트이다.**
</details>
<details> <summary>🎯 <strong>보상 모델 (Reward Model)</strong></summary>
**강화학습(RL)에서 LLM이 만든 출력이 좋은지 나쁜지를 평가하는 기준을 제공하는 모델**이다.

LLM은 스스로 "잘했나?"를 판단 못 하니까, 보상 모델이 대신 점수를 준다. 이 점수가 보상(reward)이고, 이걸 통해 LLM은 행동(출력)을 조정해나간다.

---

**📌 예시:**

**사용자가 "서울 날씨 알려줘"라고 입력했을 때,**

- A 응답: "오늘 서울은 흐리고 15도입니다." → 👍 좋은 응답 → 보상 점수 0.9
- B 응답: "서울은 대한민국의 수도입니다." → 👎 엉뚱함 → 보상 점수 0.2

→ 이런 점수 판단을 **보상 모델**이 해줘. 이걸 바탕으로 **어떤 출력을 선택해야 보상이 높아지는지 학습**하게 되는 것이다.

---

**✅ 구성 방식:**

- 대부분은 **사람이 평가한 예시 데이터**를 먼저 수집해서,
- 어떤 응답이 더 좋은지를 비교 평가한 데이터로 **ML 모델을 학습**시킨다.
- 이후 LLM이 생성한 여러 응답 중, 가장 "보상 점수가 높은" 응답을 선택하게 한다.
</details>
<details> <summary>🛠️ <strong>지도 미세조정 (SFT, Supervised Fine-Tuning)</strong></summary>
이미 **사전 학습(pretrained)된 LLM에, 사람이 만든 정답 포함 데이터셋을 추가로 학습시켜 정확도와 응답 품질을 높이는 단계**이다.

**📌 필요한 이유**

- 사전학습된 LLM은 **말은 잘하지만, 정답을 정확히 말하진 않는다.**
- 그래서 사람의 질문과 그에 대한 이상적인 응답(정답)을 보여주며 “이렇게 응답하라”고 훈련시키는 것이다.

---

**📌 예시:**

| Input | Ideal Output |
| --- | --- |
| “파이썬에서 리스트 길이 구하는 법은?” | “len(리스트이름)을 사용하면 됩니다.” |

→ 이런 데이터를 **수십만 개~수백만 개 학습**시킴.

---

**✅ 효과:**

- “적절한 응답”이 뭔지를 LLM이 **정확히 배울 수 있다.**
- **즉, “지식”보다는 “행동방식”을 맞추는 작업이라고 볼 수 있다.**
</details>
<details> <summary>🔡 <strong>토큰 (Token)</strong></summary>
**"문자열(텍스트)을 의미 있는 단위로 쪼갠 것"이다.**

### 자연어처리(NLP)/LLM 기준

- 의미: **모델이 텍스트를 처리하기 위해 분해한 최소 단위**
- 단어, 형태소, 음절, 심지어 글자까지 될 수 있음
- 예: "나는 바나나를 좋아해" → ["나는", " 바", "나", "나", "를", " 좋아", "해"]

**GPT나 BERT 같은 모델은 이 토큰 단위로 텍스트를 숫자로 바꿔서 처리하는데 이를 토크나이징(tokenizing)이라고 한다.**

### 프로그래밍 언어 처리(컴파일러 등)

- 의미: **소스코드를 의미 있는 단위로 쪼갠 결과물**
- 예: `int x = 5 + 3;` → ["int", "x", "=", "5", "+", "3", ";"]

### 보안/웹 인증에서의 토큰

- 의미: 인증 정보를 담은 **디지털 문자열**
- 예: JWT (JSON Web Token), OAuth Access Token

여기서의 토큰은 사용자 신원을 증명하는 일회성 키 같은 것이다.
</details>
<details> <summary>🔍 <strong>검색엔진 (Search Engine)</strong></summary>
**어떤 질문이나 키워드를 입력하면**, 관련된 정보를 **웹, 문서, 데이터베이스 등에서 찾아주는 시스템**이다.

**예시**

- 우리가 쓰는 **구글, 네이버, 빙, DuckDuckGo** 같은 웹 검색 서비스
- 내부적으로는:
  - 크롤링 → 웹 페이지 수집
  - 인덱싱 → 빠르게 찾기 위해 정리
  - 질의 처리 → 사용자 쿼리 분석
  - 순위 매기기 → 관련도 높은 순으로 정렬
</details>
<details> <summary>🔁 <strong>멀티턴 검색 (Multi-turn Search)</strong></summary>
**한 번 검색해서 끝나는 게 아니라**, **여러 번 검색하면서 점점 정확한 정보를 찾아가는 방식**이다.

**예시)**

1. 사용자: “세계에서 가장 큰 동물은 뭐야?”
2. LLM: “대왕고래야.”
3. 사용자: “그 동물의 평균 무게는?”
4. LLM: “평균 150톤 정도야.”

→ 이건 **대화 기반 멀티턴 검색이다**. 질문을 이어가면서 추가 정보를 요청하는 것이다.
</details>

